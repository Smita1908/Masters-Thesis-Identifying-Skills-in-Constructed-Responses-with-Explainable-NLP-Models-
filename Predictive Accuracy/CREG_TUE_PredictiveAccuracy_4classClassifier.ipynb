{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For implementation this classification task, following resources are referred:\n",
        "\n",
        "\n",
        "1.   https://huggingface.co/transformers/v3.2.0/custom_datasets.html\n",
        "2.   https://www.philschmid.de/k-fold-as-cross-validation-with-a-bert-text-classification-example\n",
        "3.   https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
        "\n"
      ],
      "metadata": {
        "id": "d674_CSbk-ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr6X-uMRk17J"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4 lxml"
      ],
      "metadata": {
        "id": "aiXmkCe5lCKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing BeautifulSoup\n",
        "from bs4 import BeautifulSoup as bs"
      ],
      "metadata": {
        "id": "4_tSvFRSlGE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "0i2IuP2jlH9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = []\n",
        "\n",
        "#loading data\n",
        "with open(\"/content/creg-tue-control-group.xml\", \"r\") as file:\n",
        "  content = file.readlines()\n",
        "content = \"\".join(content)\n",
        "bs_content = bs(content, \"xml\")"
      ],
      "metadata": {
        "id": "IOkDWtAIlIdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs_content"
      ],
      "metadata": {
        "id": "T7jkWS5jlRmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_answer_ids = []\n",
        "target_answers = []\n",
        "student_ids = []\n",
        "student_answers = []\n",
        "fourclass_labels = []"
      ],
      "metadata": {
        "id": "yF7IMcndlSA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract data from the parsed XML\n",
        "questions = bs_content.find_all('Question')\n",
        "for question in questions:\n",
        "    target_answer_id = question.find('TargetAnswer')['id']\n",
        "    target_answer = question.find('TargetAnswer').text\n",
        "\n",
        "    student_answers_data = question.find_all('StudentAnswer')\n",
        "    for student_answer_data in student_answers_data:\n",
        "        student_id = student_answer_data['student_id']\n",
        "        student_answer = student_answer_data.find('answerText').text\n",
        "        diagnoses = student_answer_data.find_all('diagnosis')\n",
        "        for diagnosis in diagnoses:\n",
        "          fourclass_value = diagnosis.get('detailed', '')\n",
        "\n",
        "          student_ids.append(student_id)\n",
        "          student_answers.append(student_answer)\n",
        "          fourclass_labels.append(fourclass_value)\n",
        "          target_answer_ids.append(target_answer_id)\n",
        "          target_answers.append(target_answer)"
      ],
      "metadata": {
        "id": "zmrzi0_flbqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'target_answer_id': target_answer_ids,\n",
        "    'target_answer': target_answers,\n",
        "    'student_id': student_ids,\n",
        "    'student_answer': student_answers,\n",
        "    'fourclass_labels': fourclass_labels\n",
        "})"
      ],
      "metadata": {
        "id": "LPKPyS2PlcZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "c5MmfCp2lequ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_skill(label):\n",
        "  skill = label\n",
        "  if skill == \"EXTRA_CONCEPT\":\n",
        "    return 3\n",
        "  elif skill == \"CORRECT\":\n",
        "    return 2\n",
        "  elif skill == \"BLEND\":\n",
        "    return 1\n",
        "  elif skill == \"MISSING_CONCEPT\":\n",
        "    return 0\n",
        "  else:\n",
        "    return 5"
      ],
      "metadata": {
        "id": "WfB3KGSnlgwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['fourclass_labels'] = df.fourclass_labels.apply(to_skill)"
      ],
      "metadata": {
        "id": "z6B1DJDNliN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['fourclass_labels'].unique()"
      ],
      "metadata": {
        "id": "r6CsdkwblkAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns = {'student_answer':'text1', 'target_answer':'text2','fourclass_labels':'labels'}, inplace = True)"
      ],
      "metadata": {
        "id": "VBuOKgq5lmPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "kHjb18R7lnkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['labels'] = df['labels'].astype(int)\n",
        "df['labels'].unique()"
      ],
      "metadata": {
        "id": "480AVLCklovh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the columns with value 5\n",
        "indices = df[df['labels'] == 5].index\n",
        "\n",
        "df = df.drop(index=indices)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "yu64rIyJlqcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reindex(columns=['text1','text2','labels','student_id','target_answer_id'])"
      ],
      "metadata": {
        "id": "9dsn46r8lwE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['labels'].unique()"
      ],
      "metadata": {
        "id": "8wMOgeOYlxkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text2'] = df['text2'].str.replace('\\n', '')\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "fA1-IZlalzUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "grR9C_htl0KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Method: Entailment-based scoring\n",
        "*   Dataset: CREG-TUE\n",
        "*   Classifier: 4-class\n",
        "\n",
        "\n",
        "For entailment-based scoring for various models, repleace the model name as follows:\n",
        "\n",
        "\n",
        "1.   FacebookAI/xlm-roberta-large\n",
        "2.   FacebookAI/xlm-roberta-base\n",
        "3.   deepset/gelectra-large\n",
        "4.   deepset/gelectra-base\n",
        "5.   deepset/gbert-large\n",
        "6.   deepset/gbert-base"
      ],
      "metadata": {
        "id": "giGXoMN-nCL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts1 = list(df['text1'].values)\n",
        "texts2 = list(df['text2'].values)\n",
        "labels = list(df['labels'].values)\n",
        "\n",
        "df1 = pd.DataFrame.from_dict({'texts1': texts1, 'texts2': texts2, 'labels': labels})\n",
        "\n",
        "class TorchDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx].clone() for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx]).long()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "#storing the individual values of f1score, accuraca, precision, recall for each fold\n",
        "f1s_list = []\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "\n",
        "for jk in range(1):\n",
        "    kfold = KFold(n_splits=5, shuffle=True)\n",
        "    l = 0\n",
        "    for train_index, val_index in kfold.split(df1):\n",
        "        print(f'{jk+1}x{l+1}')\n",
        "\n",
        "        X_train = df1[['texts1', 'texts2']].values[train_index]\n",
        "        X_test = df1[['texts1', 'texts2']].values[val_index]\n",
        "        y_train = df1['labels'].values[train_index]\n",
        "        y_test = df1['labels'].values[val_index]\n",
        "\n",
        "        train_texts = [f\"{text1} [SEP] {text2}\" for text1, text2 in zip(X_train[:, 0], X_train[:, 1])]\n",
        "        test_texts = [f\"{text1} [SEP] {text2}\" for text1, text2 in zip(X_test[:, 0], X_test[:, 1])]\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-large', truncation=True, padding=True, max_length=512)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/xlm-roberta-large\", num_labels=4, ignore_mismatched_sizes=True).to(torch.device('cuda'))\n",
        "        collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "        train_inp = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        test_inp = tokenizer(test_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        train_dataset = TorchDataSet(train_inp, y_train)\n",
        "        test_dataset = TorchDataSet(test_inp, y_test)\n",
        "\n",
        "        def compute_metrics(eval_pred):\n",
        "            labels = eval_pred.label_ids\n",
        "            preds = np.argmax(eval_pred.predictions, axis=1)\n",
        "            accuracy = accuracy_score(labels, preds)\n",
        "            f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "            precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "            recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "            return {\n",
        "                \"accuracy\": accuracy,\n",
        "                \"f1\": f1,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall\n",
        "            }\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"final_bert_trainer\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy='epoch',\n",
        "            logging_strategy='epoch',\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=1,\n",
        "            per_device_eval_batch_size=1,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            load_best_model_at_end=True,\n",
        "            save_total_limit=1,\n",
        "            logging_dir='./logs',\n",
        "            warmup_steps=400,\n",
        "            fp16=True,\n",
        "            gradient_accumulation_steps=8\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "\n",
        "        predictions = trainer.predict(test_dataset=test_dataset)\n",
        "        pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "        true_labels = predictions.label_ids\n",
        "        accuracy = accuracy_score(true_labels, pred_labels)\n",
        "        precision = precision_score(true_labels, pred_labels, average='weighted')\n",
        "        recall = recall_score(true_labels, pred_labels, average='weighted')\n",
        "        f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
        "\n",
        "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "\n",
        "\n",
        "        accuracy_list.append(accuracy)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1s_list.append(f1)\n",
        "\n",
        "# Calculating the average of evey evaluation metrics for all folds\n",
        "mean_accuracy = np.mean(accuracy_list)\n",
        "mean_precision = np.mean(precision_list)\n",
        "mean_recall = np.mean(recall_list)\n",
        "mean_f1s = np.mean(f1s_list)\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
        "print(f\"Mean Precision: {mean_precision}\")\n",
        "print(f\"Mean Recall: {mean_recall}\")\n",
        "print(f\"Mean F1 Score: {mean_f1s}\")"
      ],
      "metadata": {
        "id": "eMWVhQBjl4Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Method: Instance-based scoring\n",
        "*   Dataset: CREG-TUE\n",
        "*   Classifier: Binary\n",
        "\n",
        "\n",
        "For instance-based scoring for various models, repleace the model name as follows:\n",
        "\n",
        "\n",
        "1.   FacebookAI/xlm-roberta-large\n",
        "2.   FacebookAI/xlm-roberta-base\n",
        "3.   deepset/gelectra-large\n",
        "4.   deepset/gelectra-base\n",
        "5.   deepset/gbert-large\n",
        "6.   deepset/gbert-base"
      ],
      "metadata": {
        "id": "2NXFCZDenJJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = list(df['text1'].values)\n",
        "labels = list(df['labels'].values)\n",
        "\n",
        "df1 = pd.DataFrame.from_dict({'texts': texts, 'labels': labels})\n",
        "\n",
        "class TorchDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx].clone() for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx]).long()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "f1s_list = []\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "\n",
        "for jk in range(1):\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    l = 0\n",
        "    for train_index, val_index in kfold.split(df1):\n",
        "        print(f'{jk+1}x{l+1}')\n",
        "\n",
        "        X_train = df1['texts'].values[train_index]\n",
        "        X_test = df1['texts'].values[val_index]\n",
        "        y_train = df1['labels'].values[train_index]\n",
        "        y_test = df1['labels'].values[val_index]\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-large', truncation=True, padding=True, max_length=512)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/xlm-roberta-large\", num_labels=4, ignore_mismatched_sizes=True).to(torch.device('cuda'))\n",
        "        collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "        train_inp = tokenizer(list(X_train), padding=True, truncation=True, return_tensors='pt')\n",
        "        test_inp = tokenizer(list(X_test), padding=True, truncation=True, return_tensors='pt')\n",
        "        train = TorchDataSet(train_inp, y_train)\n",
        "        test = TorchDataSet(test_inp, y_test)\n",
        "\n",
        "        def compute_metrics(eval_pred):\n",
        "          labels = eval_pred.label_ids\n",
        "          preds = np.argmax(eval_pred.predictions, axis=1)\n",
        "          accuracy = accuracy_score(labels, preds)\n",
        "          f1 = f1_score(labels, preds, average='weighted')\n",
        "          precision = precision_score(labels, preds, average='weighted')\n",
        "          recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "          return {\n",
        "              \"accuracy\": accuracy,\n",
        "              \"f1\": f1,\n",
        "              \"precision\": precision,\n",
        "              \"recall\": recall,\n",
        "              \"quadratic_kappa\": kappa\n",
        "              }\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"final_bert_trainer\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy='epoch',\n",
        "            logging_strategy='epoch',\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=2,\n",
        "            per_device_eval_batch_size=2,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            load_best_model_at_end=True,\n",
        "            save_total_limit=1,\n",
        "            logging_dir='./logs',\n",
        "            warmup_steps=400,\n",
        "            fp16=True,\n",
        "            gradient_accumulation_steps=8\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train,\n",
        "            eval_dataset=test,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # Evaluate the model\n",
        "        predictions = trainer.predict(test_dataset=test)\n",
        "\n",
        "        pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "        true_labels = predictions.label_ids\n",
        "\n",
        "        # Compute evaluation metrics\n",
        "        accuracy = accuracy_score(true_labels, pred_labels)\n",
        "        precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
        "        recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
        "        kappa = cohen_kappa_score(true_labels, pred_labels, weights='quadratic')\n",
        "\n",
        "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}, Quadratic Kappa: {kappa}\")\n",
        "        l += 1\n",
        "\n",
        "        accuracy_list.append(accuracy)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1s_list.append(f1)\n",
        "        kappa_list.append(kappa)\n",
        "\n",
        "# Mean metric calculation for all folds\n",
        "mean_accuracy = np.mean(accuracy_list)\n",
        "mean_precision = np.mean(precision_list)\n",
        "mean_recall = np.mean(recall_list)\n",
        "mean_f1s = np.mean(f1s_list)\n",
        "mean_kappa = np.mean(kappa_list)\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
        "print(f\"Mean Precision: {mean_precision}\")\n",
        "print(f\"Mean Recall: {mean_recall}\")\n",
        "print(f\"Mean F1 Score: {mean_f1s}\")\n",
        "print(f\"Mean Quadratic Kappa: {mean_kappa}\")"
      ],
      "metadata": {
        "id": "JPoljxFAnM7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
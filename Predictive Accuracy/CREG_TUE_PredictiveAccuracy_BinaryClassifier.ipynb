{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSMJjx2UKOI_"
      },
      "source": [
        "For implementation this classification task, following resources are referred:\n",
        "\n",
        "\n",
        "1.   https://huggingface.co/transformers/v3.2.0/custom_datasets.html\n",
        "2.   https://www.philschmid.de/k-fold-as-cross-validation-with-a-bert-text-classification-example\n",
        "3.   https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEaBRtMwKFp9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJIQ-fjfKQmQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msfZtWMzKUq4"
      },
      "source": [
        "#Data scrapping using Beautiful Soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4f5VnJMKW1K"
      },
      "outputs": [],
      "source": [
        "!pip install bs4 lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0Zw-BktKXaN"
      },
      "outputs": [],
      "source": [
        "#importing BeautifulSoup\n",
        "from bs4 import BeautifulSoup as bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaQLj1-QKYzP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUEbsHX4Ka7X"
      },
      "outputs": [],
      "source": [
        "content = []\n",
        "\n",
        "#loading data\n",
        "with open(\"/content/creg-tue-control-group.xml\", \"r\") as file:\n",
        "  content = file.readlines()\n",
        "content = \"\".join(content)\n",
        "bs_content = bs(content, \"xml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL00ofrJKkuw"
      },
      "outputs": [],
      "source": [
        "bs_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FiFERFaKlUn"
      },
      "outputs": [],
      "source": [
        "s_ids = []\n",
        "answers = []\n",
        "q_ids = []\n",
        "binary_values = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd3cMSbBKpEf"
      },
      "outputs": [],
      "source": [
        "s_answers = bs_content.find_all('StudentAnswer')\n",
        "for s_ans in s_answers:\n",
        "    student_id = s_ans['student_id']\n",
        "    question_id = s_ans['question_id']\n",
        "    answer_text = s_ans.find('answerText').text\n",
        "    diagnoses = s_ans.find_all('diagnosis')\n",
        "    for diagnosis in diagnoses:\n",
        "      binary_value = diagnosis.get('binary', '')\n",
        "\n",
        "      s_ids.append(student_id)\n",
        "      q_ids.append(question_id)\n",
        "      answer.append(answer_text)\n",
        "      binary_values.append(binary_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6WqxigQKqrI"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\n",
        "    'Student_ID': s_ids,\n",
        "    'Question_ID': q_ids,\n",
        "    'Answer_Text': answer,\n",
        "    'Binary_Value': binary_values\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P2yKvWJKr2Q"
      },
      "outputs": [],
      "source": [
        "df.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXGGFNhjKtBL"
      },
      "outputs": [],
      "source": [
        "#fuction to assign anything else apart from the 0 and 1 as 2\n",
        "def to_skill(label):\n",
        "  skill = str(label).strip().lower()\n",
        "  if skill == \"true\":\n",
        "    return 1\n",
        "  elif skill == \"false\":\n",
        "    return 0\n",
        "  else:\n",
        "    return 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9IMdzTZKuMP"
      },
      "outputs": [],
      "source": [
        "df['Binary_value'] = df.Binary_Value.apply(to_skill)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xymPH3eIKvMO"
      },
      "outputs": [],
      "source": [
        "df['Binary_Value'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgGZl7IzKwTC"
      },
      "outputs": [],
      "source": [
        "df.rename(columns = {'Answer_Text':'text1', 'Binary_value':'labels'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzNh6XZRKxc8"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2K6hvkSKymo"
      },
      "outputs": [],
      "source": [
        "df['labels'] = df['labels'].astype(int)\n",
        "df['labels'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaWgNlijKz4b"
      },
      "outputs": [],
      "source": [
        "df['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpFvwowrK1Ls"
      },
      "outputs": [],
      "source": [
        "#seceting the rows with label value 2 and dropping them\n",
        "\n",
        "indices = df[df['labels'] == 2].index\n",
        "df = df.drop(index=indices)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hjP4xBJLSQA"
      },
      "outputs": [],
      "source": [
        "df['labels'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2U-aCW7LTNA"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z68g3gQ9c4fa"
      },
      "source": [
        "*   Method: Instance-based method\n",
        "*   Dataset: CREG-TUE\n",
        "*   Classifier: Binary\n",
        "\n",
        "\n",
        "For instance-based scoring for various models, repleace the model name as follows:\n",
        "\n",
        "\n",
        "1.   FacebookAI/xlm-roberta-large\n",
        "2.   FacebookAI/xlm-roberta-base\n",
        "3.   deepset/gelectra-large\n",
        "4.   deepset/gelectra-base\n",
        "5.   deepset/gbert-large\n",
        "6.   deepset/gbert-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVeF-4RPLU-B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbiO68FOLeqB"
      },
      "outputs": [],
      "source": [
        "texts = list(df['text1'].values)\n",
        "labels = list(df['labels'].values)\n",
        "\n",
        "df1 = pd.DataFrame.from_dict({'texts': texts, 'labels': labels})\n",
        "\n",
        "class TorchDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx].clone() for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx]).long()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "#storing the individual values of f1score, accuracy, precision, recall for each fold\n",
        "f1s_list = []\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "\n",
        "for jk in range(1):\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    l = 0\n",
        "    for train_index, val_index in kfold.split(df1):\n",
        "        print(f'{jk+1}x{l+1}')\n",
        "\n",
        "        X_train = df1['texts'].values[train_index]\n",
        "        X_test = df1['texts'].values[val_index]\n",
        "        y_train = df1['labels'].values[train_index]\n",
        "        y_test = df1['labels'].values[val_index]\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-large', truncation=True, padding=True, max_length=512)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/xlm-roberta-large\", num_labels=2, ignore_mismatched_sizes=True).to(torch.device('cuda'))\n",
        "        collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "        train_inp = tokenizer(list(X_train), padding=True, truncation=True, return_tensors='pt')\n",
        "        test_inp = tokenizer(list(X_test), padding=True, truncation=True, return_tensors='pt')\n",
        "        train_dataset = TorchDataSet(train_inp, y_train)\n",
        "        test_dataset = TorchDataSet(test_inp, y_test)\n",
        "\n",
        "        def compute_metrics(eval_pred):\n",
        "            labels = eval_pred.label_ids\n",
        "            preds = np.argmax(eval_pred.predictions, axis=1)\n",
        "            accuracy = accuracy_score(labels, preds)\n",
        "            f1 = f1_score(labels, preds)\n",
        "            precision = precision_score(labels, preds)\n",
        "            recall = recall_score(labels, preds)\n",
        "\n",
        "            return {\n",
        "                \"accuracy\": accuracy,\n",
        "                \"f1\": f1,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall\n",
        "\n",
        "            }\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"final_bert_trainer\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy='epoch',\n",
        "            logging_strategy='epoch',\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=2,\n",
        "            per_device_eval_batch_size=2,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            load_best_model_at_end=True,\n",
        "            save_total_limit=1,\n",
        "            logging_dir='./logs',\n",
        "            warmup_steps=400,\n",
        "            fp16=True,\n",
        "            gradient_accumulation_steps=8\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        #model_save_name = '...'\n",
        "        #path = F\".../{model_save_name}\"\n",
        "        #torch.save(model.state_dict(), path)\n",
        "\n",
        "        # Evaluate the model\n",
        "        model.eval()\n",
        "\n",
        "        logits_list = []\n",
        "        for batch in DataLoader(test_dataset, batch_size=2):\n",
        "            with torch.no_grad():\n",
        "                input_ids = batch['input_ids'].to(torch.device('cuda'))\n",
        "                attention_mask = batch['attention_mask'].to(torch.device('cuda'))\n",
        "                labels = batch['labels'].to(torch.device('cuda'))\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                logits = outputs.logits\n",
        "                logits_list.append(logits.cpu().numpy())\n",
        "\n",
        "        logits = np.concatenate(logits_list, axis=0)\n",
        "\n",
        "        threshold = 0.5\n",
        "        probabilities = torch.sigmoid(torch.tensor(logits))\n",
        "        predictions = (probabilities >= threshold).int()\n",
        "        pred = predictions.numpy()\n",
        "\n",
        "        y_test_binary = (y_test >= 0.5).astype(int)\n",
        "        pred_binary = np.argmax(pred, axis=1)\n",
        "\n",
        "        accuracy = accuracy_score(y_test_binary, pred_binary)\n",
        "        precision = precision_score(y_test_binary, pred_binary)\n",
        "        recall = recall_score(y_test_binary, pred_binary)\n",
        "        f1 = f1_score(y_test_binary, pred_binary)\n",
        "\n",
        "\n",
        "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "        l += 1\n",
        "        accuracy_list.append(accuracy)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1s_list.append(f1)\n",
        "\n",
        "\n",
        "         # clearing cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Mean metric calculation for all folds\n",
        "mean_accuracy = np.mean(accuracy_list)\n",
        "mean_precision = np.mean(precision_list)\n",
        "mean_recall = np.mean(recall_list)\n",
        "mean_f1s = np.mean(f1s_list)\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
        "print(f\"Mean Precision: {mean_precision}\")\n",
        "print(f\"Mean Recall: {mean_recall}\")\n",
        "print(f\"Mean F1 Score: {mean_f1s}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2qWKa7IONd4"
      },
      "source": [
        "Entailement_based scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6Og_BjfjSjt"
      },
      "source": [
        "*   Method: Entailment-based scoring\n",
        "*   Dataset: CREG-TUE\n",
        "*   Classifier: Binary\n",
        "\n",
        "\n",
        "For entailment-based scoring for various models, repleace the model name as follows:\n",
        "\n",
        "\n",
        "1.   FacebookAI/xlm-roberta-large\n",
        "2.   FacebookAI/xlm-roberta-base\n",
        "3.   deepset/gelectra-large\n",
        "4.   deepset/gelectra-base\n",
        "5.   deepset/gbert-large\n",
        "6.   deepset/gbert-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_oHnI7ON2N"
      },
      "outputs": [],
      "source": [
        "content = []\n",
        "\n",
        "#loading data\n",
        "with open(\"/content/creg-tue-control-group.xml\", \"r\") as file:\n",
        "  content = file.readlines()\n",
        "content = \"\".join(content)\n",
        "bs_content = bs(content, \"xml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xla60PgFdORT"
      },
      "outputs": [],
      "source": [
        "bs_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAzlIu-bdStM"
      },
      "outputs": [],
      "source": [
        "target_answer_ids = []\n",
        "question_ids = []\n",
        "target_answers = []\n",
        "student_ids = []\n",
        "student_answers = []\n",
        "twoclass_labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DejuOP8jdX0E"
      },
      "outputs": [],
      "source": [
        "q = bs_content.find_all('Question')\n",
        "for question in q:\n",
        "    target_ans_id = question.find('TargetAnswer')['id']\n",
        "    target_ans = question.find('TargetAnswer').text\n",
        "\n",
        "    student_answers_data = question.find_all('StudentAnswer')\n",
        "    for s_ans_data in student_answers_data:\n",
        "        student_id = s_ans_data['student_id']\n",
        "        question_id = s_ans_data['question_id']\n",
        "        student_answer = s_ans_data.find('answerText').text\n",
        "        diagnoses = s_ans_data.find_all('diagnosis')\n",
        "        for diagnosis in diagnoses:\n",
        "          twoclass_value = diagnosis.get('binary')\n",
        "\n",
        "          student_ids.append(student_id)\n",
        "          question_ids.append(question_id)\n",
        "          student_answers.append(student_answer)\n",
        "          twoclass_labels.append(twoclass_value)\n",
        "          target_answer_ids.append(target_ans_id)\n",
        "          target_answers.append(target_ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sje34CeNdZU6"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'target_answer_id': target_answer_ids,\n",
        "    'target_answer': target_answers,\n",
        "    'student_id': student_ids,\n",
        "    'Question_ID': question_ids,\n",
        "    'student_answer': student_answers,\n",
        "    'twoclass_labels': twoclass_labels\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y99lTQoygGG0"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw9KsCxfgOSk"
      },
      "outputs": [],
      "source": [
        "def to_skill(label):\n",
        "  skill = str(label).strip().lower()\n",
        "  if skill == \"true\":\n",
        "    return 1\n",
        "  elif skill == \"false\":\n",
        "    return 0\n",
        "  else:\n",
        "    return 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msqWCUyLgQA2"
      },
      "outputs": [],
      "source": [
        "df['twoclass_labels'] = df.twoclass_labels.apply(to_skill)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC9DFKlIgSUk"
      },
      "outputs": [],
      "source": [
        "df['twoclass_labels'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8OQSaj8gT7O"
      },
      "outputs": [],
      "source": [
        "df.rename(columns = {'student_answer':'text1', 'target_answer':'text2', 'twoclass_labels':'labels'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOCnyEkkgVnh"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nplXgijngW9q"
      },
      "outputs": [],
      "source": [
        "df['labels'] = df['labels'].astype(int)\n",
        "df['labels'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_dxG_KFgZGW"
      },
      "outputs": [],
      "source": [
        "# dropping the records with label 2\n",
        "indices = df[df['labels'] == 2].index\n",
        "df = df.drop(index=indices)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpWlXmGtgjKk"
      },
      "outputs": [],
      "source": [
        "df = df.reindex(columns=['text1','text2','labels','student_id','target_answer_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM9yL9rsgkjM"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1gNhDLoglJL"
      },
      "outputs": [],
      "source": [
        "df['text2'] = df['text2'].str.replace('\\n', '')#removing the front \\n from every text\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CLshxUXOLbn"
      },
      "outputs": [],
      "source": [
        "#student response, reference answer and labels\n",
        "texts1 = list(df['text1'].values)\n",
        "texts2 = list(df['text2'].values)\n",
        "labels = list(df['labels'].values)\n",
        "\n",
        "df1 = pd.DataFrame.from_dict({'texts1': texts1, 'texts2': texts2, 'labels': labels})\n",
        "\n",
        "class TorchDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx].clone() for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx]).long()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "#storing the individual values of f1score, accuraca, precision, recall for each fold\n",
        "f1s_list = []\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "kappa_list = []\n",
        "\n",
        "for jk in range(1):\n",
        "    kfold = KFold(n_splits=5, shuffle=True)\n",
        "    l = 0\n",
        "    for train_index, val_index in kfold.split(df1):\n",
        "        print(f'{jk+1}x{l+1}')\n",
        "\n",
        "        X_train = df1[['texts1', 'texts2']].values[train_index]\n",
        "        X_test = df1[['texts1', 'texts2']].values[val_index]\n",
        "        y_train = df1['labels'].values[train_index]\n",
        "        y_test = df1['labels'].values[val_index]\n",
        "\n",
        "        train_texts = [f\"{text1} [SEP] {text2}\" for text1, text2 in zip(X_train[:, 0], X_train[:, 1])]\n",
        "        test_texts = [f\"{text1} [SEP] {text2}\" for text1, text2 in zip(X_test[:, 0], X_test[:, 1])]\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained('deepset/gelectra-large', truncation=True, padding=True, max_length=512)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"deepset/gelectra-large\", num_labels=2, ignore_mismatched_sizes=True).to(torch.device('cuda'))\n",
        "        collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "        train_inp = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        test_inp = tokenizer(test_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        train_dataset = TorchDataSet(train_inp, y_train)\n",
        "        test_dataset = TorchDataSet(test_inp, y_test)\n",
        "\n",
        "        def compute_metrics(eval_pred):\n",
        "            labels = eval_pred.label_ids\n",
        "            preds = np.argmax(eval_pred.predictions, axis=1)\n",
        "            accuracy = accuracy_score(labels, preds)\n",
        "            f1 = f1_score(labels, preds)\n",
        "            precision = precision_score(labels, preds)\n",
        "            recall = recall_score(labels, preds)\n",
        "            return {\n",
        "                \"accuracy\": accuracy,\n",
        "                \"f1\": f1,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall\n",
        "            }\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"final_bert_trainer\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy='epoch',\n",
        "            logging_strategy='epoch',\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=1,\n",
        "            per_device_eval_batch_size=1,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            load_best_model_at_end=True,\n",
        "            save_total_limit=1,\n",
        "            logging_dir='./logs',\n",
        "            warmup_steps=400,\n",
        "            fp16=True,\n",
        "            gradient_accumulation_steps=8\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "\n",
        "        predictions = trainer.predict(test_dataset=test_dataset)\n",
        "        pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "        true_labels = predictions.label_ids\n",
        "\n",
        "        accuracy = accuracy_score(true_labels, pred_labels)\n",
        "        precision = precision_score(true_labels, pred_labels)\n",
        "        recall = recall_score(true_labels, pred_labels)\n",
        "        f1 = f1_score(true_labels, pred_labels)\n",
        "\n",
        "\n",
        "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "        l += 1\n",
        "\n",
        "        accuracy_list.append(accuracy)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1s_list.append(f1)\n",
        "        kappa_list.append(kappa)\n",
        "\n",
        "# Calculating the average of evey evaluation metrics for all folds\n",
        "mean_accuracy = np.mean(accuracy_list)\n",
        "mean_precision = np.mean(precision_list)\n",
        "mean_recall = np.mean(recall_list)\n",
        "mean_f1s = np.mean(f1s_list)\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
        "print(f\"Mean Precision: {mean_precision}\")\n",
        "print(f\"Mean Recall: {mean_recall}\")\n",
        "print(f\"Mean F1 Score: {mean_f1s}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For implementation this classification task, following resources are referred:\n",
        "\n",
        "\n",
        "1.   https://huggingface.co/transformers/v3.2.0/custom_datasets.html\n",
        "2.   https://www.philschmid.de/k-fold-as-cross-validation-with-a-bert-text-classification-example"
      ],
      "metadata": {
        "id": "5kxc-A-QOdgv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK6dpSdEOZ71"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "_hTrghj5OlnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "6ZfL3ZRlOowm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "df = pd.read_excel('/content/Skill_with_question_id.xlsx')\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "GvyJGcs9Oq9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Constructing_Explanations'].replace('-','2', inplace = True)# replacing entry \"-\" with 2\n",
        "df = df[~df['Constructing_Explanations'].isnull()] # Constructing_Explanations contained NaN\n",
        "df = df[df['Constructing_Explanations'].str.isnumeric()] # changing to numerical value\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "S1rFKAO9OvDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to check for value 1 and 0 for labels\n",
        "def to_skill(label):\n",
        "    skill = int(label)\n",
        "    if skill == 1:\n",
        "        return 1\n",
        "    elif skill == 0:\n",
        "        return 0\n",
        "\n",
        "df['Constructing_Explanations'] = df.Constructing_Explanations.apply(to_skill)\n",
        "df = df.dropna()#droping null value"
      ],
      "metadata": {
        "id": "gbkshaGvPYN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={'Answer': 'text1', 'Solution': 'text2', 'Constructing_Explanations': 'labels'}, inplace=True) #renaming the necessary columns\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "tsl-8uYSPZxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "4PZ1qQQmPa-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Method: Entailment-based\n",
        "*   Skill: Constructing Explanations\n",
        "*   Dataset: AFLEK\n",
        "*   Models with prediction saved\n",
        "\n",
        "\n",
        "For entailment-based scoring for various models, repleace the model name as follows:\n",
        "\n",
        "\n",
        "1.   FacebookAI/xlm-roberta-large\n",
        "2.   FacebookAI/xlm-roberta-base\n",
        "3.   deepset/gelectra-large\n",
        "4.   deepset/gelectra-base\n",
        "5.   deepset/gbert-large\n",
        "6.   deepset/gbert-base"
      ],
      "metadata": {
        "id": "jQbhFQ8HPg9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts1 = list(df['text1'].values)\n",
        "texts2 = list(df['text2'].values)\n",
        "labels = list(df['labels'].values)\n",
        "students = list(df['Student'].values)\n",
        "q_ids = list(df['question_id'].values)\n",
        "\n",
        "df1 = pd.DataFrame.from_dict({'texts1': texts1, 'texts2': texts2, 'labels': labels, 'students': students, 'question_ids': q_ids})\n",
        "\n",
        "class TorchDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx].clone() for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx]).long()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "#storing the individual values of f1score, accuraca, precision, recall for each fold\n",
        "f1s_list = []\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "\n",
        "\n",
        "# storing all the predictions, ground truths, and other details\n",
        "Full_predictions = []\n",
        "Full_ground_truths = []\n",
        "all_students = []\n",
        "all_solutions = []\n",
        "all_answers = []\n",
        "all_question_ids = []\n",
        "\n",
        "for jk in range(1):\n",
        "    kfold = KFold(n_splits=5, shuffle=True)\n",
        "    l = 0\n",
        "    for train_index, val_index in kfold.split(df1):\n",
        "        print(f'{jk+1}x{l+1}')\n",
        "\n",
        "        X_train = df1[['texts1', 'texts2']].values[train_index]\n",
        "        X_test = df1[['texts1', 'texts2']].values[val_index]\n",
        "        y_train = df1['labels'].values[train_index]\n",
        "        y_test = df1['labels'].values[val_index]\n",
        "        student_ids_test = df1['students'].values[val_index]\n",
        "        solutions_test = df1['texts2'].values[val_index]\n",
        "        answers_test = df1['texts1'].values[val_index]\n",
        "        question_ids_test = df1['question_ids'].values[val_index]\n",
        "\n",
        "        train_texts = [f\"{text1} [SEP] {text2}\" for text1, text2 in zip(X_train[:, 0], X_train[:, 1])]\n",
        "        test_texts = [f\"{text1} [SEP] {text2}\" for text1, text2 in zip(X_test[:, 0], X_test[:, 1])]\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained('deepset/gbert-large', truncation=True, padding=True, max_length=512)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"deepset/gbert-large\", num_labels=2, ignore_mismatched_sizes=True).to(torch.device('cuda'))\n",
        "        collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "        train_inp = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        test_inp = tokenizer(test_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        train_dataset = TorchDataSet(train_inp, y_train)\n",
        "        test_dataset = TorchDataSet(test_inp, y_test)\n",
        "\n",
        "        def compute_metrics(eval_pred):\n",
        "            labels = eval_pred.label_ids\n",
        "            preds = np.argmax(eval_pred.predictions, axis=1)\n",
        "            accuracy = accuracy_score(labels, preds)\n",
        "            f1 = f1_score(labels, preds)\n",
        "            precision = precision_score(labels, preds)\n",
        "            recall = recall_score(labels, preds)\n",
        "\n",
        "            return {\n",
        "                \"accuracy\": accuracy,\n",
        "                \"f1\": f1,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall\n",
        "            }\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"final_bert_trainer\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy='epoch',\n",
        "            logging_strategy='epoch',\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=2,\n",
        "            per_device_eval_batch_size=2,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            load_best_model_at_end=True,\n",
        "            save_total_limit=1,\n",
        "            logging_dir='./logs',\n",
        "            warmup_steps=400,\n",
        "            fp16=True,\n",
        "            gradient_accumulation_steps=8\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        #model_save_name = 'FT_CE_EB_GBERTLarge.pth'\n",
        "        #path = F\".../{model_save_name}\"\n",
        "        #torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        logits_list = []\n",
        "        for test_text in test_texts:\n",
        "            with torch.no_grad():\n",
        "                test_input = tokenizer(test_text, padding=True, truncation=True, return_tensors='pt')\n",
        "                test_input_tensor = {k: v.to(torch.device('cuda')) for k, v in test_input.items()}\n",
        "                logits = model(**test_input_tensor).logits\n",
        "                logits_list.append(logits.cpu().numpy())\n",
        "        logits = np.concatenate(logits_list, axis=0)\n",
        "\n",
        "        threshold = 0.5\n",
        "        probabilities = torch.sigmoid(torch.tensor(logits))\n",
        "        predictions = (probabilities >= threshold).int()\n",
        "        pred = predictions.numpy()\n",
        "\n",
        "        y_test_binary = (y_test >= 0.5).astype(int)\n",
        "        pred_binary = np.argmax(pred, axis=1)\n",
        "\n",
        "         # saving the prediction along with other details\n",
        "        Full_predictions.extend(pred_binary)\n",
        "        Full_ground_truths.extend(y_test_binary)\n",
        "        all_students.extend(student_ids_test)\n",
        "        all_solutions.extend(solutions_test)\n",
        "        all_answers.extend(answers_test)\n",
        "        all_question_ids.extend(question_ids_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test_binary, pred_binary)\n",
        "        precision = precision_score(y_test_binary, pred_binary)\n",
        "        recall = recall_score(y_test_binary, pred_binary)\n",
        "        f1 = f1_score(y_test_binary, pred_binary)\n",
        "\n",
        "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "        l += 1\n",
        "        accuracy_list.append(accuracy)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1s_list.append(f1)\n",
        "\n",
        "\n",
        "        # claring cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Calculating the average of evey evaluation metrics for all folds\n",
        "mean_accuracy = np.mean(accuracy_list)\n",
        "mean_precision = np.mean(precision_list)\n",
        "mean_recall = np.mean(recall_list)\n",
        "mean_f1s = np.mean(f1s_list)\n",
        "\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
        "print(f\"Mean Precision: {mean_precision}\")\n",
        "print(f\"Mean Recall: {mean_recall}\")\n",
        "print(f\"Mean F1 Score: {mean_f1s}\")\n",
        "\n",
        "\n",
        "# Saving to the predictions to outputfille\n",
        "output_df = pd.DataFrame({\n",
        "    'student_id': all_students,\n",
        "    'question_id': all_question_ids,\n",
        "    'Solution': all_solutions,\n",
        "    'Answer': all_answers,\n",
        "    'Actual Label': Full_ground_truths,\n",
        "    'Predicted Label': Full_predictions\n",
        "})\n",
        "output_df.to_excel('../predictions_CE_EB_GBERT_large.xlsx', index=False)"
      ],
      "metadata": {
        "id": "F1dCg2btPiRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Method: Instance-based method\n",
        "*   Skill: Constructing Explanations\n",
        "*   Dataset: AFLEK\n",
        "*   Models with prediction saved\n",
        "\n",
        "\n",
        "For instance-based scoring for various models, repleace the model name as follows:\n",
        "\n",
        "\n",
        "1.   FacebookAI/xlm-roberta-large\n",
        "2.   FacebookAI/xlm-roberta-base\n",
        "3.   deepset/gelectra-large\n",
        "4.   deepset/gelectra-base\n",
        "5.   deepset/gbert-large\n",
        "6.   deepset/gbert-base"
      ],
      "metadata": {
        "id": "bL3Kj_bXbITi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#only require the student response and labels\n",
        "texts1 = list(df['text1'].values)\n",
        "labels = list(df['labels'].values)\n",
        "\n",
        "df1 = pd.DataFrame.from_dict({'texts1': texts1, 'labels': labels})\n",
        "\n",
        "class TorchDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx].clone() for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx]).long()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "#storing the individual values of f1score, accuraca, precision, recall for each fold\n",
        "f1s_list = []\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "\n",
        "\n",
        "for jk in range(1):\n",
        "    kfold = KFold(n_splits=5, shuffle=True)\n",
        "    l = 0\n",
        "    for train_index, val_index in kfold.split(df1):\n",
        "        print(f'{jk+1}x{l+1}')\n",
        "\n",
        "        X_train = df1['texts1'].values[train_index]\n",
        "        X_test = df1['texts1'].values[val_index]\n",
        "        y_train = df1['labels'].values[train_index]\n",
        "        y_test = df1['labels'].values[val_index]\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained('deepset/gelectra-large', truncation=True, padding=True, max_length=512)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"deepset/gelectra-large\", num_labels=2, ignore_mismatched_sizes=True).to(torch.device('cuda'))\n",
        "        collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "        train_inp = tokenizer(list(X_train), padding=True, truncation=True, return_tensors='pt')\n",
        "        test_inp = tokenizer(list(X_test), padding=True, truncation=True, return_tensors='pt')\n",
        "        train = TorchDataSet(train_inp, y_train)\n",
        "        test = TorchDataSet(test_inp, y_test)\n",
        "\n",
        "        def compute_metrics(eval_pred):\n",
        "            labels = eval_pred.label_ids\n",
        "            preds = np.argmax(eval_pred.predictions, axis=1)\n",
        "            accuracy = accuracy_score(labels, preds)\n",
        "            f1 = f1_score(labels, preds)\n",
        "            precision = precision_score(labels, preds)\n",
        "            recall = recall_score(labels, preds)\n",
        "            kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n",
        "            return {\n",
        "                \"accuracy\": accuracy,\n",
        "                \"f1\": f1,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall\n",
        "\n",
        "            }\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"final_bert_trainer\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy='epoch',\n",
        "            logging_strategy='epoch',\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=2,\n",
        "            per_device_eval_batch_size=2,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            load_best_model_at_end=True,\n",
        "            save_total_limit=1,\n",
        "            logging_dir='./logs',\n",
        "            warmup_steps=400,\n",
        "            fp16=True,\n",
        "            gradient_accumulation_steps=8\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train,\n",
        "            eval_dataset=test,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        logits_list = []\n",
        "        for batch in DataLoader(test, batch_size=2):\n",
        "          with torch.no_grad():\n",
        "            input_ids = batch['input_ids'].to(torch.device('cuda'))\n",
        "            attention_mask = batch['attention_mask'].to(torch.device('cuda'))\n",
        "            labels = batch['labels'].to(torch.device('cuda'))\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            logits = outputs.logits\n",
        "            logits_list.append(logits.cpu().numpy())\n",
        "\n",
        "        logits = np.concatenate(logits_list, axis=0)\n",
        "\n",
        "        threshold = 0.5\n",
        "        probabilities = torch.sigmoid(torch.tensor(logits))\n",
        "        predictions = (probabilities >= threshold).int()\n",
        "        pred = predictions.numpy()\n",
        "\n",
        "        y_test_binary = (y_test >= 0.5).astype(int)\n",
        "        pred_binary = np.argmax(pred, axis=1)\n",
        "\n",
        "        accuracy = accuracy_score(y_test_binary, pred_binary)\n",
        "        precision = precision_score(y_test_binary, pred_binary)\n",
        "        recall = recall_score(y_test_binary, pred_binary)\n",
        "        f1 = f1_score(y_test_binary, pred_binary)\n",
        "\n",
        "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "        l += 1\n",
        "        accuracy_list.append(accuracy)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1s_list.append(f1)\n",
        "\n",
        "         # claring cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Calculating the average of evey evaluation metrics for all folds\n",
        "mean_accuracy = np.mean(accuracy_list)\n",
        "mean_precision = np.mean(precision_list)\n",
        "mean_recall = np.mean(recall_list)\n",
        "mean_f1s = np.mean(f1s_list)\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
        "print(f\"Mean Precision: {mean_precision}\")\n",
        "print(f\"Mean Recall: {mean_recall}\")\n",
        "print(f\"Mean F1 Score: {mean_f1s}\")"
      ],
      "metadata": {
        "id": "rCh3WxWPbgjG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
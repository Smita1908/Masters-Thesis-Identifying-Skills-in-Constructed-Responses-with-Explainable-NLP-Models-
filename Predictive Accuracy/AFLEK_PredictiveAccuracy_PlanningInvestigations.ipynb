{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For implementation this classification task, following resources are referred:\n",
        "\n",
        "\n",
        "1.   https://huggingface.co/transformers/v3.2.0/custom_datasets.html\n",
        "2.   https://www.philschmid.de/k-fold-as-cross-validation-with-a-bert-text-classification-example"
      ],
      "metadata": {
        "id": "Y2Ea3OYbeFU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64S1v7pmeBCN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "MQdnFtfQeIhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading data from the skill file\n",
        "df = pd.read_excel('/content/Skill_with_question_id.xlsx')\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "5hki9SOmeKV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Planning_Investigations'].replace('-','2', inplace = True) # replacing entry \"-\" with 2\n",
        "df = df[~df['Planning_Investigations'].isnull()]#checking for null value\n",
        "df = df[df['Planning_Investigations'].str.isnumeric()]# changing to numerical value\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "fUur3nBWeYP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to check for value 1 and 0 for labels\n",
        "def to_skill(label):\n",
        "    skill = int(label)\n",
        "    if skill == 1:\n",
        "        return 1\n",
        "    elif skill == 0:\n",
        "        return 0\n",
        "\n",
        "df['Planning_Investigations'] = df.Planning_Investigations.apply(to_skill)\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "6El1-F5ieZZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns={'Answer': 'text1', 'Solution': 'text2', 'Planning_Investigations': 'labels'}, inplace=True)#renaming the necessary columns\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "m-Pstl8LebZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reindex(columns=['text1', 'text2', 'labels','Student', 'question_id','Constructing_Explanations', 'Analyzing_Data'])#reindexing for easier use\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "7LLuLxHXeb7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Method: Instance-based\n",
        "*   Skill: Planning_Investigationa\n",
        "*   Dataset: AFLEK\n",
        "*   Models with prediction saved\n",
        "\n",
        "\n",
        "For instance-based scoring for various models, repleace the model name as follows:\n",
        "\n",
        "\n",
        "1.   FacebookAI/xlm-roberta-large\n",
        "2.   FacebookAI/xlm-roberta-base\n",
        "3.   deepset/gelectra-large\n",
        "4.   deepset/gelectra-base\n",
        "5.   deepset/gbert-large\n",
        "6.   deepset/gbert-base"
      ],
      "metadata": {
        "id": "YsBQjNoUexVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = list(df['text1'].values)\n",
        "labels = list(df['labels'].values)\n",
        "students = list(df['Student'].values)\n",
        "sols = list(df['Solution'].values)\n",
        "q_ids = list(df['question_id'].values)\n",
        "\n",
        "df1 = pd.DataFrame.from_dict({'texts': texts, 'labels': labels, 'students': students, 'solutions': sols, 'question_ids': q_ids})\n",
        "\n",
        "class TorchDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx]).long()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "#storing the individual values of f1score, accuraca, precision, recall for each fold\n",
        "f1s_list = []\n",
        "acc_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "\n",
        "\n",
        "# storing all the predictions, ground truths, and other details\n",
        "all_predictions = []\n",
        "all_ground_truths = []\n",
        "all_students = []\n",
        "all_solutions = []\n",
        "all_answers = []\n",
        "all_question_ids = []\n",
        "\n",
        "for jk in range(1):\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    l = 0\n",
        "    for train_index, val_index in kfold.split(df1):\n",
        "        print(f'{jk+1}x{l+1}')\n",
        "\n",
        "        X_train = df1['texts'].values[train_index]\n",
        "        X_test = df1['texts'].values[val_index]\n",
        "        y_train = df1['labels'].values[train_index]\n",
        "        y_test = df1['labels'].values[val_index]\n",
        "        student_ids_test = df1['students'].values[val_index]\n",
        "        solutions_test = df1['solutions'].values[val_index]\n",
        "        answers_test = df1['texts'].values[val_index]\n",
        "        question_ids_test = df1['question_ids'].values[val_index]\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained('deepset/gbert-base', truncation=True, padding=True, max_length=512)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"deepset/gbert-base\", num_labels=2, ignore_mismatched_sizes=True).to(torch.device('cuda'))\n",
        "        collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "        train_inp = tokenizer(list(X_train), padding=True, truncation=True, return_tensors='pt')\n",
        "        test_inp = tokenizer(list(X_test), padding=True, truncation=True, return_tensors='pt')\n",
        "        train_dataset = TorchDataSet(train_inp, y_train)\n",
        "        test_dataset = TorchDataSet(test_inp, y_test)\n",
        "\n",
        "        def compute_metrics(eval_pred):\n",
        "            labels = eval_pred.label_ids\n",
        "            preds = np.argmax(eval_pred.predictions, axis=1)\n",
        "            accuracy = accuracy_score(labels, preds)\n",
        "            f1 = f1_score(labels, preds)\n",
        "            precision = precision_score(labels, preds)\n",
        "            recall = recall_score(labels, preds)\n",
        "            return {\n",
        "                \"accuracy\": accuracy,\n",
        "                \"f1\": f1,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall\n",
        "            }\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"final_bert_trainer\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy='epoch',\n",
        "            logging_strategy='epoch',\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=2,\n",
        "            per_device_eval_batch_size=2,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            load_best_model_at_end=True,\n",
        "            save_total_limit=1,\n",
        "            logging_dir='./logs',\n",
        "            warmup_steps=400,\n",
        "            fp16=True,\n",
        "            gradient_accumulation_steps=8\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        #model_save_name = '..'\n",
        "        #path = F\"../{model_save_name}\"\n",
        "        #torch.save(model.state_dict(), path)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        logits_list = []\n",
        "        for batch in DataLoader(test_dataset, batch_size=2):\n",
        "            with torch.no_grad():\n",
        "                input_ids = batch['input_ids'].to(torch.device('cuda'))\n",
        "                attention_mask = batch['attention_mask'].to(torch.device('cuda'))\n",
        "                labels = batch['labels'].to(torch.device('cuda'))\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                logits = outputs.logits\n",
        "                logits_list.append(logits.cpu().numpy())\n",
        "\n",
        "        logits = np.concatenate(logits_list, axis=0)\n",
        "\n",
        "        threshold = 0.5\n",
        "        probabilities = torch.sigmoid(torch.tensor(logits))\n",
        "        predictions = (probabilities >= threshold).int()\n",
        "        pred = predictions.numpy()\n",
        "\n",
        "        y_test_binary = (y_test >= 0.5).astype(int)\n",
        "        pred_binary = np.argmax(pred, axis=1)\n",
        "\n",
        "        # saving the prediction along with other details\n",
        "        all_predictions.extend(pred_binary)\n",
        "        all_ground_truths.extend(y_test_binary)\n",
        "        all_students.extend(student_ids_test)\n",
        "        all_solutions.extend(solutions_test)\n",
        "        all_answers.extend(answers_test)\n",
        "        all_question_ids.extend(question_ids_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test_binary, pred_binary)\n",
        "        precision = precision_score(y_test_binary, pred_binary)\n",
        "        recall = recall_score(y_test_binary,pred_binary)\n",
        "        f1 = f1_score(y_test_binary, pred_binary)\n",
        "\n",
        "\n",
        "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "        l += 1\n",
        "        acc_list.append(accuracy)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1s_list.append(f1)\n",
        "\n",
        "\n",
        "        # claring cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Calculating the average of evey evaluation metrics for all folds\n",
        "mean_acc = np.mean(acc_list)\n",
        "mean_precision = np.mean(precision_list)\n",
        "mean_recall = np.mean(recall_list)\n",
        "mean_f1s = np.mean(f1s_list)\n",
        "\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_acc}\")\n",
        "print(f\"Mean Precision: {mean_precision}\")\n",
        "print(f\"Mean Recall: {mean_recall}\")\n",
        "print(f\"Mean F1 Score: {mean_f1s}\")\n",
        "\n",
        "# Saving to the predictions to outputfile\n",
        "output_df = pd.DataFrame({\n",
        "    'student_id': all_students,\n",
        "    'question_id': all_question_ids,\n",
        "    'Solution': all_solutions,\n",
        "    'Answer': all_answers,\n",
        "    'Actual Label': all_ground_truths,\n",
        "    'Predicted Label': all_predictions\n",
        "})\n",
        "#output_df.to_excel('...xlsx', index=False)\n",
        "\n",
        "#print(\"Predictions and ground truths have been saved to 'predictions_ground_truths.xlsx'.\")"
      ],
      "metadata": {
        "id": "7ytx-b0Wew-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Method: Entailment-based\n",
        "*   Skill: Planning Investigations\n",
        "*   Dataset: AFLEK\n",
        "*   Models with prediction saved\n",
        "\n",
        "\n",
        "For entailment-based scoring for various models, repleace the model name as follows:\n",
        "\n",
        "\n",
        "1.   FacebookAI/xlm-roberta-large\n",
        "2.   FacebookAI/xlm-roberta-base\n",
        "3.   deepset/gelectra-large\n",
        "4.   deepset/gelectra-base\n",
        "5.   deepset/gbert-large\n",
        "6.   deepset/gbert-base"
      ],
      "metadata": {
        "id": "sovBteMigTKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts1 = list(df['text1'].values)\n",
        "texts2 = list(df['text2'].values)\n",
        "labels = list(df['labels'].values)\n",
        "students = list(df['Student'].values)\n",
        "sols = list(df['text2'].values)\n",
        "q_ids = list(df['question_id'].values)\n",
        "\n",
        "df1 = pd.DataFrame.from_dict({'texts1': texts1, 'texts2': texts2, 'labels': labels, 'students': students, 'solutions': sols, 'question_ids': q_ids})\n",
        "\n",
        "class TorchDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: v[idx].clone() for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx]).long()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "#storing the individual values of f1score, accuraca, precision, recall for each fold\n",
        "f1s_list = []\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "\n",
        "\n",
        "# storing all the predictions, ground truths, and other details\n",
        "Full_predictions = []\n",
        "Full_ground_truths = []\n",
        "all_students = []\n",
        "all_solutions = []\n",
        "all_answers = []\n",
        "all_question_ids = []\n",
        "\n",
        "for jk in range(1):\n",
        "    kfold = KFold(n_splits=5, shuffle=True)\n",
        "    l = 0\n",
        "    for train_index, val_index in kfold.split(df1):\n",
        "        print(f'{jk+1}x{l+1}')\n",
        "\n",
        "        X_train = df1[['texts1', 'texts2']].values[train_index]\n",
        "        X_test = df1[['texts1', 'texts2']].values[val_index]\n",
        "        y_train = df1['labels'].values[train_index]\n",
        "        y_test = df1['labels'].values[val_index]\n",
        "        student_ids_test = df1['students'].values[val_index]\n",
        "        solutions_test = df1['solutions'].values[val_index]\n",
        "        answers_test = df1['texts1'].values[val_index]\n",
        "        question_ids_test = df1['question_ids'].values[val_index]\n",
        "\n",
        "        train_texts = [f\"{text1} [SEP] {text2}\" for text1, text2 in zip(X_train[:, 0], X_train[:, 1])]\n",
        "        test_texts = [f\"{text1} [SEP] {text2}\" for text1, text2 in zip(X_test[:, 0], X_test[:, 1])]\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-large', truncation=True, padding=True, max_length=512)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/xlm-roberta-large\", num_labels=2, ignore_mismatched_sizes=True).to(torch.device('cuda'))\n",
        "        collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "        train_inp = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        test_inp = tokenizer(test_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        train_dataset = TorchDataSet(train_inp, y_train)\n",
        "        test_dataset = TorchDataSet(test_inp, y_test)\n",
        "\n",
        "        def compute_metrics(eval_pred):\n",
        "            labels = eval_pred.label_ids\n",
        "            preds = np.argmax(eval_pred.predictions, axis=1)\n",
        "            accuracy = accuracy_score(labels, preds)\n",
        "            f1 = f1_score(labels, preds)\n",
        "            precision = precision_score(labels, preds)\n",
        "            recall = recall_score(labels, preds)\n",
        "\n",
        "            return {\n",
        "                \"accuracy\": accuracy,\n",
        "                \"f1\": f1,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall\n",
        "            }\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"final_bert_trainer\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy='epoch',\n",
        "            logging_strategy='epoch',\n",
        "            learning_rate=1e-5,\n",
        "            per_device_train_batch_size=2,\n",
        "            per_device_eval_batch_size=2,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=0.01,\n",
        "            load_best_model_at_end=True,\n",
        "            save_total_limit=1,\n",
        "            logging_dir='./logs',\n",
        "            warmup_steps=400,\n",
        "            fp16=True,\n",
        "            gradient_accumulation_steps=8\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        #model_save_name = '...'\n",
        "        #path = F\".../{model_save_name}\"\n",
        "        #torch.save(model.state_dict(), path)\n",
        "\n",
        "        model.eval()\n",
        "        logits_list = []\n",
        "        for batch in DataLoader(test_dataset, batch_size = 2):\n",
        "            with torch.no_grad():\n",
        "              input_ids = batch['input_ids'].to(torch.device('cuda'))\n",
        "              attention_mask = batch['attention_mask'].to(torch.device('cuda'))\n",
        "              labels = batch['labels'].to(torch.device('cuda'))\n",
        "\n",
        "              outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "              logits = outputs.logits\n",
        "              logits_list.append(logits.cpu().numpy())\n",
        "        logits = np.concatenate(logits_list, axis=0)\n",
        "\n",
        "        threshold = 0.5\n",
        "        probabilities = torch.sigmoid(torch.tensor(logits))\n",
        "        predictions = (probabilities >= threshold).int()\n",
        "        pred = predictions.numpy()\n",
        "\n",
        "        y_test_binary = (y_test >= 0.5).astype(int)\n",
        "        pred_binary = np.argmax(pred, axis=1)\n",
        "\n",
        "        # saving the prediction along with other details\n",
        "        full_predictions.extend(pred_binary)\n",
        "        full_ground_truths.extend(y_test_binary)\n",
        "        all_students.extend(student_ids_test)\n",
        "        all_solutions.extend(solutions_test)\n",
        "        all_answers.extend(answers_test)\n",
        "        all_question_ids.extend(question_ids_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test_binary, pred_binary)\n",
        "        precision = precision_score(y_test_binary, pred_binary)\n",
        "        recall = recall_score(y_test_binary, pred_binary)\n",
        "        f1 = f1_score(y_test_binary, pred_binary)\n",
        "\n",
        "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "        l += 1\n",
        "        accuracy_list.append(accuracy)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1s_list.append(f1)\n",
        "\n",
        "        # claring cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Calculating the average of evey evaluation metrics for all folds\n",
        "mean_accuracy = np.mean(accuracy_list)\n",
        "mean_precision = np.mean(precision_list)\n",
        "mean_recall = np.mean(recall_list)\n",
        "mean_f1s = np.mean(f1s_list)\n",
        "\n",
        "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
        "print(f\"Mean Precision: {mean_precision}\")\n",
        "print(f\"Mean Recall: {mean_recall}\")\n",
        "print(f\"Mean F1 Score: {mean_f1s}\")\n",
        "\n",
        "# Saving to the predictions to outputfille\n",
        "output_df = pd.DataFrame({\n",
        "    'student_id': all_students,\n",
        "    'question_id': all_question_ids,\n",
        "    'Solution': all_solutions,\n",
        "    'Answer': all_answers,\n",
        "    'Actual Label': full_ground_truths,\n",
        "    'Predicted Label': full_predictions\n",
        "})\n",
        "#output_df.to_excel('...', index=False)\n",
        "\n",
        "print(\"Predictions and ground truths have been saved to 'predictions_ground_truths.xlsx'.\")"
      ],
      "metadata": {
        "id": "0bUSuLT5gS2Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
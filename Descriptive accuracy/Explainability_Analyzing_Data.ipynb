{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For implementation of this local and global explanations, following resource is referred:\n",
        "\n",
        "\n",
        "1.   https://shap-lrjball.readthedocs.io/en/latest/example_notebooks/general/Explainable%20AI%20with%20Shapley%20Values.html"
      ],
      "metadata": {
        "id": "FshTWmBWiPxh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRN0jD5tiO9M"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "PjJ2fjYFi5si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "aBkY9Q8ei6T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data from the skill file\n",
        "df = pd.read_excel(\"/content/Skill_with_question_id.xlsx\")\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "xUo4OsuOi9jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reindex(columns=['Answer', 'Analyzing_Data', 'Solution', 'Student', 'Constructing_Explanations', 'Planning_Investigations', 'question_id'])\n",
        "df['Analyzing_Data'].replace('-', '2', inplace=True)\n",
        "df = df[~df['Analyzing_Data'].isnull()]\n",
        "df = df[df['Analyzing_Data'].str.isnumeric()]"
      ],
      "metadata": {
        "id": "4T8nVf8z0o4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_skill(label):\n",
        "    skill = int(label)\n",
        "    if skill == 1:\n",
        "        return 1\n",
        "    elif skill == 0:\n",
        "        return 0\n",
        "\n",
        "df['Analyzing_Data'] = df.Analyzing_Data.apply(to_skill)\n",
        "df = df.dropna()\n",
        "df.rename(columns={'Answer': 'text', 'Solution':'text2', 'Analyzing_Data': 'label'}, inplace=True)\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "wZX-eC5TjmSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "orrZ0ksyjrRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df.iloc[:474] # splitting the dataframe rowwise with first 50% of the data\n",
        "df_train_pd_whole = df_train.iloc[:,0:2]# splitting the training data columnwise and taking only text and label columns\n",
        "df_train_pd_whole.head(5)"
      ],
      "metadata": {
        "id": "Xp5B7vCC0y2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_dict(df_train_pd_whole) # converting the dataframe into datasets.arrow_dataset.Dataset"
      ],
      "metadata": {
        "id": "P26rehg703HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_val = df.iloc[475:711] #spliting data row-wise 25% of the data for validation set\n",
        "df_val_pd_whole = df_val.iloc[:,0:2]# spliting the valdation dataset columnwise only to take text and the label\n",
        "validation_dataset = Dataset.from_dict(df_val_pd_whole) # converting the dataframe into datasets.arrow_dataset.Dataset"
      ],
      "metadata": {
        "id": "DS9-jfsz08Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df.iloc[711:] #spliting data row-wise 25% of the data for test set\n",
        "df_test_pd_whole = df_test.iloc[:,0:2]# spliting the test dataset columnwise only to take text and the label\n",
        "test_dataset = Dataset.from_dict(df_val_pd_whole) # converting the dataframe into datasets.arrow_dataset.Dataset"
      ],
      "metadata": {
        "id": "Fpw29dRPju4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting datasets.arrow_dataset.Dataset into datasets.dataset_dict.DatasetDict'\n",
        "final_dataset_dict = datasets.DatasetDict({\"train\":train_dataset,\"test\":test_dataset, \"validation\":validation_dataset})\n",
        "final_dataset_dict"
      ],
      "metadata": {
        "id": "FBRukOMo09tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final_dataset_dict = final_dataset_dict[\"train\"]\n",
        "train_final_dataset_dict"
      ],
      "metadata": {
        "id": "83f_qkoI0_1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final_dataset_dict = final_dataset_dict[\"train\"]"
      ],
      "metadata": {
        "id": "F7fouU1lj2TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "hzi3n0vij3oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import Dataset\n",
        "import shap"
      ],
      "metadata": {
        "id": "w6AkNtCOj5AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_AD_EB_GBERTLarge = AutoModelForSequenceClassification.from_pretrained(\"deepset/gbert-large\", num_labels=2, ignore_mismatched_sizes=True).to(torch.device('cuda'))\n",
        "model_AD_EB_GBERTLarge.load_state_dict(torch.load('/content/gdrive/MyDrive/Thesis/Model/FT_AD_EB_GBERTLarge.pth'))"
      ],
      "metadata": {
        "id": "RsQMw40Tj6nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_AD_EB_GBERTLarge"
      ],
      "metadata": {
        "id": "AFN32c7aj8gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('deepset/gbert-large', truncation=True, padding=True, max_length=512)"
      ],
      "metadata": {
        "id": "7YL9vIf8kAne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a prediction function\n",
        "def f(x):\n",
        "    encodings = [tokenizer.encode_plus(v, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\") for v in x]\n",
        "    input_ids = torch.cat([e['input_ids'] for e in encodings], dim=0).cuda()\n",
        "    attention_mask = torch.cat([e['attention_mask'] for e in encodings], dim=0).cuda()\n",
        "\n",
        "    tv = torch.tensor(input_ids).cuda()\n",
        "    #sourceTensor.clone().detach()\n",
        "    outputs = model_AD_EB_GBERTLarge(tv, attention_mask=attention_mask)[0].detach().cpu().numpy()\n",
        "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
        "    val = sp.special.logit(scores[:, 1])  # using one vs rest of the logits available\n",
        "    return val\n"
      ],
      "metadata": {
        "id": "ZP7-SfYb1Ryb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an explainer using a token masker\n",
        "explainer = shap.Explainer(f, tokenizer)"
      ],
      "metadata": {
        "id": "7D7VI3zbkYre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final_dataset_dict[:50] #checking the format"
      ],
      "metadata": {
        "id": "UCsTrcmOk1fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explaining the model's prediction on the Analyzing Data skill of AFLEK data\n",
        "shap_values = explainer(train_final_dataset_dict[:474], fixed_context=1, batch_size=2)"
      ],
      "metadata": {
        "id": "safJHcq1k77A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_values"
      ],
      "metadata": {
        "id": "bC2GvzU41lsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "global positive"
      ],
      "metadata": {
        "id": "rkig59mUqdUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "s_value = shap_values.values\n",
        "d_value = shap_values.data\n",
        "\n",
        "# selecing the features from the shap_values\n",
        "feature_Names = [list(data) for data in d_value]\n",
        "\n",
        "# Flattening SHAP values and feature names\n",
        "flattened_s_value = []\n",
        "flattened_feature = []\n",
        "\n",
        "for shap_vals, feat_names in zip(s_value, feature_Names):\n",
        "    flattened_s_value.extend(shap_vals)\n",
        "    flattened_feature.extend(feat_names)\n",
        "\n",
        "# storing the highest shap value for each unique feature\n",
        "pos_feature_shap_dict = {}\n",
        "seen_features = set()\n",
        "\n",
        "for feature, sv in zip(flattened_feature, flattened_s_value):\n",
        "    if feature:  #chekcing for blank features\n",
        "        if feature in seen_features:\n",
        "            continue  # skipping the feaures if it is already there to have a unique feature set\n",
        "        seen_features.add(feature)\n",
        "        if sv > 0:\n",
        "            if feature in pos_feature_shap_dict:\n",
        "                pos_feature_shap_dict[feature] = max(pos_feature_shap_dict[feature], sv)\n",
        "            else:\n",
        "                pos_feature_shap_dict[feature] = sv\n",
        "\n",
        "# sorting features with max shap value in descening order\n",
        "sorted_pos_f = sorted(pos_feature_shap_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# function for plotting different number of features\n",
        "def plotting_top_pos_f(n):\n",
        "    top_n_pos_f = sorted_pos_f[:n]\n",
        "    top_n_pos_fnames = [feature for feature, value in top_n_pos_f]\n",
        "    top_n_pos_s_values = [value for feature, value in top_n_positive_features]\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.bar(top_n_pos_fnames, top_n_pos_s_values, color='#ff0052')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel('Maximum SHAP Value (Positive Contributions)')\n",
        "    plt.xlabel('Features')\n",
        "    plt.title(f'Top {n} Unique Features Contributing to Positive Class')\n",
        "    plt.show()\n",
        "\n",
        "# Top 30\n",
        "plotting_top_pos_f(30)"
      ],
      "metadata": {
        "id": "ZVx2ooKOlIoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "global negative"
      ],
      "metadata": {
        "id": "c65A_X-Wqfqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "s_value = shap_values.values\n",
        "d_value = shap_values.data\n",
        "\n",
        "# selecing the features from the shap_values\n",
        "feature_Names = [list(data) for data in d_value]\n",
        "\n",
        "# Flattening SHAP values and feature names\n",
        "flattened_s_value = []\n",
        "flattened_feature = []\n",
        "\n",
        "for shap_vals, feat_names in zip(s_value, feature_Names):\n",
        "    flattened_s_value.extend(shap_vals)\n",
        "    flattened_feature.extend(feat_names)\n",
        "\n",
        "# storing the negative shap value for each unique feature\n",
        "neg_feature_shap_dict = {}\n",
        "seen_features = set()\n",
        "\n",
        "for feature, sv in zip(flattened_feature, flattened_s_value):\n",
        "    if feature:  #chekcing for blank features\n",
        "        if feature in seen_features:\n",
        "            continue  # skipping the feaures if it is already there to have a unique feature set\n",
        "        seen_features.add(feature)\n",
        "        if sv < 0:\n",
        "            if feature in neg_feature_shap_dict:\n",
        "                neg_feature_shap_dict[feature] = min(neg_feature_shap_dict[feature], sv) # because we wanted to check the minimun negative features to compare with the positive ones\n",
        "            else:\n",
        "                neg_feature_shap_dict[feature] = sv\n",
        "\n",
        "# sorting features with min shap value\n",
        "sorted_neg_features = sorted(neg_feature_shap_dict.items(), key=lambda x: x[1])\n",
        "\n",
        "# selecting the bottom 30 unique features contributing to the negative class\n",
        "top_30 = sorted_neg_features[:30]\n",
        "top_30_neg_f_names = [feature for feature, value in top_30]\n",
        "top_30_neg_sv = [value for feature, value in top_30]\n",
        "\n",
        "# plotting the featurs\n",
        "color_blue = '#1e88e5'\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(top_30_neg_f_names, top_30_neg_sv, color=color_blue)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel('Minimum SHAP Value (Negative Contributions)')\n",
        "plt.xlabel('Features')\n",
        "plt.title('Top 30 Unique Features Contributing to Negative Class')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tdSdbWNOqg58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For local explanations, pass the specific response index number for the following text plot, force plot and waterfall plot"
      ],
      "metadata": {
        "id": "z73tNJ5wwYDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.text(shap_values[2])"
      ],
      "metadata": {
        "id": "x9GTglJ9waGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()\n",
        "shap.force_plot(shap_values[2].base_values, shap_values[2].values, shap_values[2].data)"
      ],
      "metadata": {
        "id": "2XeI9OzewdRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.waterfall(shap_values[2])"
      ],
      "metadata": {
        "id": "yYTuk9bIwhax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Occlusion study"
      ],
      "metadata": {
        "id": "VhuN4_XplQ0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generation of the occluded dataset"
      ],
      "metadata": {
        "id": "gYX-Qq8Oxqqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# the evidence span is supplied separately in a json file\n",
        "with open('/content/assembled.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "records = []\n",
        "\n",
        "#iterating for every record in the data\n",
        "for q_id, q_data in data.items():\n",
        "    for s_id, s_data in q_data['answers'].items():\n",
        "        # Checking if the label of the Analyzing data score is present(file structure)\n",
        "        if 'labels' in s_data and 'Analyzing data' in s_data['labels']:\n",
        "            label = 1 if s_data['labels']['Analyzing data'].get('score') == 'present' else 0\n",
        "            tokens_list = s_data.get('tokens', [])\n",
        "            e_list = s_data['labels']['Analyzing data'].get('evidences', [])\n",
        "\n",
        "            combined_sentence = []\n",
        "            # processing for all the token for a sentence in response\n",
        "            for sentence_tokens, sentence_evidences in zip(tokens_list, e_list):\n",
        "                if label == 1:\n",
        "                    # Masking the tokens if the evidence score is 1\n",
        "                    masked_sentence = [\n",
        "                        '[MASK]' if ev == 1 else tok for tok, ev in zip(sentence_tokens, sentence_evidences)\n",
        "                    ]\n",
        "                else:\n",
        "                    # copying the unchanged response if there is no evidence span\n",
        "                    masked_sentence = sentence_tokens\n",
        "\n",
        "                combined_sentence.extend(masked_sentence)\n",
        "\n",
        "            final_sentence = \" \".join(combined_sentence)\n",
        "            records.append({\n",
        "                'StudentID': student_id,\n",
        "                'Masked Sentence': final_sentence,\n",
        "                'Label': label\n",
        "            })\n",
        "\n",
        "# saving the newly created data\n",
        "df_records = pd.DataFrame(records)\n",
        "excel_file_path = 'processed_data.xlsx'\n",
        "df_records.to_excel(excel_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "7w-HPw9Oxu1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_excel(\"/content/processed_data.xlsx\")\n",
        "df1.head(5)"
      ],
      "metadata": {
        "id": "s6qBF4kalTEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Label'].replace('-','2', inplace = True)\n",
        "df1 = df1[~df1['Label'].isnull()] # checking for null\n",
        "df1['Label'].dtype"
      ],
      "metadata": {
        "id": "QNN7AzAelV7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_skill(label):\n",
        "    skill = int(label)\n",
        "    if skill == 1:\n",
        "        return 1\n",
        "    elif skill == 0:\n",
        "        return 0\n",
        "\n",
        "df1['Label'] = df1.Label.apply(to_skill)\n",
        "df1 = df1.dropna()"
      ],
      "metadata": {
        "id": "-TOOjAMolaHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.rename(columns = {'Masked Sentence':'text','Label':'label'}, inplace = True) # renaming it"
      ],
      "metadata": {
        "id": "mwEI7mVqlbyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "Yqm8x53B2Fzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_train = df1.iloc[:474] # splitting the dataframe rowwise with first 50% of the data\n",
        "df1_train_pd_whole = df1_train.iloc[:,1:3]# splitting the training data columnwise and taking only text and label columns\n",
        "df1_train_pd_whole.head(5)"
      ],
      "metadata": {
        "id": "0QDAGXBzlke-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train1_dataset = Dataset.from_dict(df1_train_pd_whole)"
      ],
      "metadata": {
        "id": "k1lYMTPL2JjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_val = df1.iloc[475:711] #spliting data row-wise 25% of the data for validation set\n",
        "df1_val_pd_whole = df1_val.iloc[:,1:3]# spliting the valdation dataset columnwise only to take text and the label\n",
        "validation1_dataset = Dataset.from_dict(df1_val_pd_whole) # converting the dataframe into datasets.arrow_dataset.Dataset"
      ],
      "metadata": {
        "id": "fCweOrXm2KMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_test = df1.iloc[711:] #spliting data row-wise 25% of the data for test set\n",
        "df1_test_pd_whole = df1_test.iloc[:,1:3]# spliting the test dataset columnwise only to take text and the label\n",
        "test1_dataset = Dataset.from_dict(df1_test_pd_whole) # converting the dataframe into datasets.arrow_dataset.Dataset"
      ],
      "metadata": {
        "id": "_0wotnMD2N4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting datasets.arrow_dataset.Dataset into datasets.dataset_dict.DatasetDict'\n",
        "final_dataset_dict1 = datasets.DatasetDict({\"train\":train1_dataset,\"test\":test1_dataset, \"validation\":validation1_dataset})\n",
        "final_dataset_dict1"
      ],
      "metadata": {
        "id": "XU2rS8nE2Pew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final_dataset_dict = final_dataset_dict[\"train\"]"
      ],
      "metadata": {
        "id": "wAGnU78TlmNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "aHBqmomR2TrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import Dataset\n",
        "import shap"
      ],
      "metadata": {
        "id": "zAF0mKCP2hRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_AD_EB_GBERTLarge = AutoModelForSequenceClassification.from_pretrained(\"deepset/gbert-large\", num_labels=2, ignore_mismatched_sizes=True).to(torch.device('cuda'))\n",
        "model_AD_EB_GBERTLarge.load_state_dict(torch.load('/content/gdrive/MyDrive/Thesis/Model/FT_AD_EB_GBERTLarge.pth'))"
      ],
      "metadata": {
        "id": "ulL1WKH8lpcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('deepset/gbert-large', truncation=True, padding=True, max_length=512)"
      ],
      "metadata": {
        "id": "6Awa2Wlxltc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a prediction function\n",
        "def f(x):\n",
        "    encodings = [tokenizer.encode_plus(v, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\") for v in x]\n",
        "    input_ids = torch.cat([e['input_ids'] for e in encodings], dim=0).cuda()\n",
        "    attention_mask = torch.cat([e['attention_mask'] for e in encodings], dim=0).cuda()\n",
        "\n",
        "    tv = torch.tensor(input_ids).cuda()\n",
        "    #sourceTensor.clone().detach()\n",
        "    outputs = model_AD_EB_GBERTLarge(tv, attention_mask=attention_mask)[0].detach().cpu().numpy()\n",
        "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
        "    val = sp.special.logit(scores[:, 1])  # using one vs rest of the logits available\n",
        "    return val"
      ],
      "metadata": {
        "id": "VCWDBnljlt5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build an explainer using a token masker\n",
        "explainer = shap.Explainer(f, tokenizer)"
      ],
      "metadata": {
        "id": "2iqABYPolwJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final_dataset_dict[:50]"
      ],
      "metadata": {
        "id": "3fJvJbhXlxb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_values = explainer(train_final_dataset_dict[:2000], fixed_context=1, batch_size=2)"
      ],
      "metadata": {
        "id": "Th6S_uVxlz91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Global positive"
      ],
      "metadata": {
        "id": "D8wVVIscmQJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "s_value = shap_values.values\n",
        "d_value = shap_values.data\n",
        "\n",
        "# selecing the features from the shap_values\n",
        "feature_Names = [list(data) for data in d_value]\n",
        "\n",
        "# Flatten SHAP values and feature names\n",
        "flattened_s_value = []\n",
        "flattened_feature = []\n",
        "\n",
        "for shap_vals, feat_names in zip(s_value, feature_Names):\n",
        "    flattened_s_value.extend(shap_vals)\n",
        "    flattened_feature.extend(feat_names)\n",
        "\n",
        "# storing the highest shap value for each unique feature\n",
        "pos_feature_shap_dict = {}\n",
        "seen_features = set()\n",
        "\n",
        "for feature, s_value in zip(flattened_feature, flattened_s_value):\n",
        "    if feature:  #chekcing for blank features\n",
        "        if feature in seen_features:\n",
        "            continue  # skipping the feaures if it is already there to have a unique feature set\n",
        "        seen_features.add(feature)\n",
        "        if s_value > 0:\n",
        "            if feature in pos_feature_shap_dict:\n",
        "                pos_feature_shap_dict[feature] = max(pos_feature_shap_dict[feature], s_value)\n",
        "            else:\n",
        "                pos_feature_shap_dict[feature] = s_value\n",
        "\n",
        "# sorting features with max shap value in descening order\n",
        "sorted_pos_f = sorted(pos_feature_shap_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# function for plotting different number of features\n",
        "def plotting_top_pos_f(n):\n",
        "    top_n_pos_f = sorted_pos_f[:n]\n",
        "    top_n_pos_fnames = [feature for feature, value in top_n_pos_f]\n",
        "    top_n_pos_s_values = [value for feature, value in top_n_positive_features]\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.bar(top_n_pos_fnames, top_n_pos_s_values, color='#ff0052')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel('Maximum SHAP Value (Positive Contributions)')\n",
        "    plt.xlabel('Features')\n",
        "    plt.title(f'Top {n} Unique Features Contributing to Positive Class')\n",
        "    plt.show()\n",
        "\n",
        "# Top 30\n",
        "plotting_top_pos_f(30)\n"
      ],
      "metadata": {
        "id": "jsxfjKgymSRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##global negative"
      ],
      "metadata": {
        "id": "dCxdUuaWwRTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "s_value = shap_values.values\n",
        "d_value = shap_values.data\n",
        "\n",
        "# selecing the features from the shap_values\n",
        "feature_Names = [list(data) for data in d_value]\n",
        "\n",
        "# Flattening SHAP values and feature names\n",
        "flattened_s_value = []\n",
        "flattened_feature = []\n",
        "\n",
        "for shap_vals, feat_names in zip(s_value, feature_Names):\n",
        "    flattened_s_value.extend(shap_vals)\n",
        "    flattened_feature.extend(feat_names)\n",
        "\n",
        "# storing the negative shap value for each unique feature\n",
        "neg_feature_shap_dict = {}\n",
        "seen_features = set()\n",
        "\n",
        "for feature, sv in zip(flattened_feature, flattened_s_value):\n",
        "    if feature:  #chekcing for blank features\n",
        "        if feature in seen_features:\n",
        "            continue  # skipping the feaures if it is already there to have a unique feature set\n",
        "        seen_features.add(feature)\n",
        "        if sv < 0:\n",
        "            if feature in neg_feature_shap_dict:\n",
        "                neg_feature_shap_dict[feature] = min(neg_feature_shap_dict[feature], sv) # because we wanted to check the minimun negative features to compare with the positive ones\n",
        "            else:\n",
        "                neg_feature_shap_dict[feature] = sv\n",
        "\n",
        "# sorting features with min shap value\n",
        "sorted_neg_features = sorted(neg_feature_shap_dict.items(), key=lambda x: x[1])\n",
        "\n",
        "# selecting the bottom 30 unique features contributing to the negative class\n",
        "top_30 = sorted_neg_features[:30]\n",
        "top_30_neg_f_names = [feature for feature, value in top_30]\n",
        "top_30_neg_sv = [value for feature, value in top_30]\n",
        "\n",
        "# plotting the featurs\n",
        "color_blue = '#1e88e5'\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(top_30_neg_f_names, top_30_neg_sv, color=color_blue)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel('Minimum SHAP Value (Negative Contributions)')\n",
        "plt.xlabel('Features')\n",
        "plt.title('Top 30 Unique Features Contributing to Negative Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H6RoRDXqwStV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Local Explanations\n",
        "we can put the specific response index number for the following text plot, force plot and waterfall plot\n"
      ],
      "metadata": {
        "id": "r-OalT2ow16S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.text(shap_values[15])"
      ],
      "metadata": {
        "id": "6zSr_xpHw9Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()\n",
        "shap.force_plot(shap_values[15].base_values, shap_values[15].values, shap_values[15].data)"
      ],
      "metadata": {
        "id": "bjZGQZq1w6Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.waterfall(shap_values[15])"
      ],
      "metadata": {
        "id": "GVnQ04tFw2iz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For implementation of this local and global explanations, following resource is referred:\n",
        "\n",
        "\n",
        "1.   https://shap-lrjball.readthedocs.io/en/latest/example_notebooks/general/Explainable%20AI%20with%20Shapley%20Values.html"
      ],
      "metadata": {
        "id": "FshTWmBWiPxh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRN0jD5tiO9M"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "PjJ2fjYFi5si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "aBkY9Q8ei6T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data from the skill file\n",
        "df = pd.read_excel(\"/content/Skill_with_question_id.xlsx\")\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "xUo4OsuOi9jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Planning_Investigations'].replace('-','2', inplace = True)\n",
        "df = df[~df['Planning_Investigations'].isnull()] # checkiing for null\n",
        "df = df[df['Planning_Investigations'].str.isnumeric()]\n",
        "df.head(5)\n",
        "\n",
        "def to_skill(label):\n",
        "    skill = int(label)\n",
        "    if skill == 1:\n",
        "        return 1\n",
        "    elif skill == 0:\n",
        "        return 0\n",
        "\n",
        "df['Planning_Investigations'] = df.Planning_Investigations.apply(to_skill)\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "wZX-eC5TjmSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reindex(columns=['Answer','Planning_Investigations','Solution','Student','Constructing_Explanations','Analyzing_Data'])\n",
        "df.rename(columns = {'Answer':'text','Planning_Investigations':'label'}, inplace = True)"
      ],
      "metadata": {
        "id": "1pCSOXTQjqqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "orrZ0ksyjrRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df.iloc[:78] # splitting the dataframe rowwise with first 50% of the data\n",
        "df_train_pd_whole = df_train.iloc[:,0:2]# splitting the training data columnwise and taking only text1,text2 and label columns\n",
        "train_dataset = Dataset.from_dict(df_train_pd_whole) # converting the dataframe into datasets.arrow_dataset.Dataset\n",
        "\n",
        "df_val = df.iloc[79:117] #spliting data row-wise 25% of the data for validation set\n",
        "df_val_pd_whole = df_val.iloc[:,0:2]# spliting the valdation dataset columnwise only to take text and the label\n",
        "validation_dataset = Dataset.from_dict(df_val_pd_whole) # converting the dataframe into datasets.arrow_dataset.Dataset\n",
        "\n",
        "df_test = df.iloc[117:] #spliting data row-wise 25% of the data for test set\n",
        "df_test_pd_whole = df_test.iloc[:,0:2]# spliting the test dataset columnwise only to take text and the label\n",
        "test_dataset = Dataset.from_dict(df_test_pd_whole) # converting the dataframe into datasets.arrow_dataset.Dataset\n",
        "\n",
        "#converting datasets.arrow_dataset.Dataset into datasets.dataset_dict.DatasetDict'\n",
        "final_dataset_dict = datasets.DatasetDict({\"train\":train_dataset,\"test\":test_dataset, \"validation\":validation_dataset})\n",
        "final_dataset_dict"
      ],
      "metadata": {
        "id": "Fpw29dRPju4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final_dataset_dict = final_dataset_dict[\"train\"]"
      ],
      "metadata": {
        "id": "F7fouU1lj2TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "hzi3n0vij3oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import Dataset\n",
        "import shap"
      ],
      "metadata": {
        "id": "w6AkNtCOj5AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_PI_EB_GELECTRALarge = AutoModelForSequenceClassification.from_pretrained(\"deepset/gelectra-large\", num_labels=2, ignore_mismatched_sizes=True).to(torch.device('cuda'))\n",
        "model_PI_EB_GELECTRALarge.load_state_dict(torch.load('/content/gdrive/MyDrive/Thesis/Models/FT_PI_EB_gelectraLarge.pth'))"
      ],
      "metadata": {
        "id": "RsQMw40Tj6nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_PI_EB_GELECTRALarge"
      ],
      "metadata": {
        "id": "AFN32c7aj8gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('deepset/gelectra-large', truncation=True, padding=True, max_length=512)"
      ],
      "metadata": {
        "id": "7YL9vIf8kAne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a prediction function\n",
        "def f(x):\n",
        "    encodings = [tokenizer.encode_plus(v, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\") for v in x]\n",
        "    input_ids = torch.cat([e['input_ids'] for e in encodings], dim=0).cuda()\n",
        "    attention_mask = torch.cat([e['attention_mask'] for e in encodings], dim=0).cuda()\n",
        "\n",
        "    tv = torch.tensor(input_ids).cuda()\n",
        "    outputs = model_PI_EB_GELECTRALarge(tv, attention_mask=attention_mask)[0].detach().cpu().numpy()\n",
        "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
        "    val = sp.special.logit(scores[:, 1])  # using one vs rest of the logits available\n",
        "    return val\n"
      ],
      "metadata": {
        "id": "eRzjgSEOkGIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an explainer using a token masker\n",
        "explainer = shap.Explainer(f, tokenizer)"
      ],
      "metadata": {
        "id": "7D7VI3zbkYre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final_dataset_dict[:50] #checking"
      ],
      "metadata": {
        "id": "UCsTrcmOk1fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explaining the model's prediction on the Constructing Explanations skill of AFLEK data\n",
        "shap_values = explainer(train_final_dataset_dict[:78], fixed_context=1, batch_size=2)"
      ],
      "metadata": {
        "id": "safJHcq1k77A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_values"
      ],
      "metadata": {
        "id": "ZJQNFu818eF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "global positive"
      ],
      "metadata": {
        "id": "rkig59mUqdUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "s_value = shap_values.values\n",
        "d_value = shap_values.data\n",
        "\n",
        "# selecing the features from the shap_values\n",
        "feature_Names = [list(data) for data in d_value]\n",
        "\n",
        "# Flattening SHAP values and feature names\n",
        "flattened_s_value = []\n",
        "flattened_feature = []\n",
        "\n",
        "for shap_vals, feat_names in zip(s_value, feature_Names):\n",
        "    flattened_s_value.extend(shap_vals)\n",
        "    flattened_feature.extend(feat_names)\n",
        "\n",
        "# storing the highest shap value for each unique feature\n",
        "pos_feature_shap_dict = {}\n",
        "seen_features = set()\n",
        "\n",
        "for feature, sv in zip(flattened_feature, flattened_s_value):\n",
        "    if feature:  #chekcing for blank features\n",
        "        if feature in seen_features:\n",
        "            continue  # skipping the feaures if it is already there to have a unique feature set\n",
        "        seen_features.add(feature)\n",
        "        if sv > 0:\n",
        "            if feature in pos_feature_shap_dict:\n",
        "                pos_feature_shap_dict[feature] = max(pos_feature_shap_dict[feature], sv)\n",
        "            else:\n",
        "                pos_feature_shap_dict[feature] = sv\n",
        "\n",
        "# sorting features with max shap value in descening order\n",
        "sorted_pos_f = sorted(pos_feature_shap_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# function for plotting different number of features\n",
        "def plotting_top_pos_f(n):\n",
        "    top_n_pos_f = sorted_pos_f[:n]\n",
        "    top_n_pos_fnames = [feature for feature, value in top_n_pos_f]\n",
        "    top_n_pos_s_values = [value for feature, value in top_n_positive_features]\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.bar(top_n_pos_fnames, top_n_pos_s_values, color='#ff0052')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel('Maximum SHAP Value (Positive Contributions)')\n",
        "    plt.xlabel('Features')\n",
        "    plt.title(f'Top {n} Unique Features Contributing to Positive Class')\n",
        "    plt.show()\n",
        "\n",
        "# Top 30\n",
        "plotting_top_pos_f(30)"
      ],
      "metadata": {
        "id": "ZVx2ooKOlIoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "global negative"
      ],
      "metadata": {
        "id": "c65A_X-Wqfqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "s_value = shap_values.values\n",
        "d_value = shap_values.data\n",
        "\n",
        "# selecing the features from the shap_values\n",
        "feature_Names = [list(data) for data in d_value]\n",
        "\n",
        "# Flattening SHAP values and feature names\n",
        "flattened_s_value = []\n",
        "flattened_feature = []\n",
        "\n",
        "for shap_vals, feat_names in zip(s_value, feature_Names):\n",
        "    flattened_s_value.extend(shap_vals)\n",
        "    flattened_feature.extend(feat_names)\n",
        "\n",
        "# storing the negative shap value for each unique feature\n",
        "neg_feature_shap_dict = {}\n",
        "seen_features = set()\n",
        "\n",
        "for feature, sv in zip(flattened_feature, flattened_s_value):\n",
        "    if feature:  #chekcing for blank features\n",
        "        if feature in seen_features:\n",
        "            continue  # skipping the feaures if it is already there to have a unique feature set\n",
        "        seen_features.add(feature)\n",
        "        if sv < 0:\n",
        "            if feature in neg_feature_shap_dict:\n",
        "                neg_feature_shap_dict[feature] = min(neg_feature_shap_dict[feature], sv) # because we wanted to check the minimun negative features to compare with the positive ones\n",
        "            else:\n",
        "                neg_feature_shap_dict[feature] = sv\n",
        "\n",
        "# sorting features with min shap value\n",
        "sorted_neg_features = sorted(neg_feature_shap_dict.items(), key=lambda x: x[1])\n",
        "\n",
        "# selecting the bottom 30 unique features contributing to the negative class\n",
        "top_30 = sorted_neg_features[:30]\n",
        "top_30_neg_f_names = [feature for feature, value in top_30]\n",
        "top_30_neg_sv = [value for feature, value in top_30]\n",
        "\n",
        "# plotting the featurs\n",
        "color_blue = '#1e88e5'\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(top_30_neg_f_names, top_30_neg_sv, color=color_blue)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel('Minimum SHAP Value (Negative Contributions)')\n",
        "plt.xlabel('Features')\n",
        "plt.title('Top 30 Unique Features Contributing to Negative Class')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tdSdbWNOqg58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Local Explanations\n",
        "we can put the specific response index number for the following text plot, force plot and waterfall plot"
      ],
      "metadata": {
        "id": "z73tNJ5wwYDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.text(shap_values[3])"
      ],
      "metadata": {
        "id": "x9GTglJ9waGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.initjs()\n",
        "shap.force_plot(shap_values[3].base_values, shap_values[3].values, shap_values[3].data)"
      ],
      "metadata": {
        "id": "2XeI9OzewdRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.plots.waterfall(shap_values[3])"
      ],
      "metadata": {
        "id": "yYTuk9bIwhax"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}